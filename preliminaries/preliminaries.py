#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Entry point: run the upstream `src/eval.ipynb` as a plain Python program.

Default behavior:
  1) Download the upstream repository source into ./workspace/in-context-learning (if missing)
  2) Download model checkpoints into ./workspace/in-context-learning/models (if missing)
  3) Export src/eval.ipynb into generated Python files under ./workspace/generated
  4) Execute the merged script (cells run in notebook order)

Usage:
  python preliminaries.py
  python preliminaries.py --export-only
  python preliminaries.py --skip-models
  python preliminaries.py --skip-download  (requires an existing repo in workspace)
"""

from __future__ import annotations

import argparse
import json
import os
import re
import runpy
import shutil
import sys
import types
import urllib.request
import zipfile
from pathlib import Path
from typing import Any, List, Tuple


UPSTREAM_REPO_ZIP = "https://codeload.github.com/dtsip/in-context-learning/zip/refs/heads/main"
UPSTREAM_MODELS_ZIP = "https://github.com/dtsip/in-context-learning/releases/download/initial/models.zip"


MAGIC_LINE_RE = re.compile(r"^(\s*)([%!].*)$")


def _coerce_source(src: Any) -> str:
    if src is None:
        return ""
    if isinstance(src, str):
        return src
    if isinstance(src, list):
        return "".join(src)
    return str(src)


def _comment_out_magics(code: str) -> str:
    """
    Convert Jupyter magics (%..., !..., %%...) into comments so the code can run in Python.
    """
    out_lines: List[str] = []
    for line in code.splitlines(True):  # keep line endings
        m = MAGIC_LINE_RE.match(line)
        if m:
            indent, payload = m.group(1), m.group(2).rstrip("\n")
            out_lines.append(f"{indent}# {payload}\n")
        else:
            out_lines.append(line)
    return "".join(out_lines)


def export_notebook(nb_path: Path, out_root: Path) -> Tuple[Path, List[Path]]:
    """
    Export notebook to:
      - merged script: out_root/eval_from_ipynb.py
      - parts: out_root/eval_ipynb_parts/{idx}_{type}.py
    Returns (merged_path, part_paths).
    """
    data = json.loads(nb_path.read_text(encoding="utf-8"))
    cells = data.get("cells", [])

    out_root.mkdir(parents=True, exist_ok=True)
    parts_dir = out_root / "eval_ipynb_parts"
    parts_dir.mkdir(parents=True, exist_ok=True)

    merged_path = out_root / "eval_from_ipynb.py"
    part_paths: List[Path] = []

    merged_lines: List[str] = []
    merged_lines.append("# Auto-generated from src/eval.ipynb\n")
    merged_lines.append("# Generated by preliminaries.py\n\n")

    for idx, cell in enumerate(cells):
        ctype = cell.get("cell_type", "unknown")
        src = _coerce_source(cell.get("source", ""))

        header = f"# ===== cell {idx:03d} ({ctype}) =====\n"
        merged_lines.append(header)

        part_name = f"{idx:03d}_{ctype}.py"
        part_path = parts_dir / part_name
        part_paths.append(part_path)

        if ctype == "markdown":
            block = [header]
            for ln in src.splitlines():
                block.append("# " + ln + "\n")
            if not src.endswith("\n"):
                block.append("\n")
            block.append("\n")

            part_path.write_text("".join(block), encoding="utf-8")
            merged_lines.extend(block[1:])  # avoid duplicate header

        elif ctype == "code":
            code = _comment_out_magics(src)
            if not code.endswith("\n"):
                code += "\n"

            part_path.write_text(header + "\n" + code + "\n", encoding="utf-8")
            merged_lines.append("\n" + code + "\n")

        else:
            block = [header, f"# [skipped unsupported cell_type={ctype!r}]\n\n"]
            part_path.write_text("".join(block), encoding="utf-8")
            merged_lines.extend(block[1:])

        merged_lines.append("\n")

    merged_path.write_text("".join(merged_lines), encoding="utf-8")
    return merged_path, part_paths


def _download(url: str, dest: Path) -> None:
    dest.parent.mkdir(parents=True, exist_ok=True)
    tmp = dest.with_suffix(dest.suffix + ".tmp")

    def reporthook(blocknum: int, blocksize: int, totalsize: int) -> None:
        if totalsize <= 0:
            return
        read = min(blocknum * blocksize, totalsize)
        pct = (read / totalsize) * 100
        end = "\n" if read >= totalsize else "\r"
        print(f"Downloading {dest.name}: {pct:6.2f}% ({read}/{totalsize} bytes)", end=end)

    try:
        urllib.request.urlretrieve(url, tmp, reporthook=reporthook)
        tmp.replace(dest)
    finally:
        if tmp.exists():
            tmp.unlink(missing_ok=True)


def _extract_zip(zip_path: Path, dest_dir: Path) -> None:
    dest_dir.mkdir(parents=True, exist_ok=True)
    with zipfile.ZipFile(zip_path, "r") as zf:
        zf.extractall(dest_dir)


def _ensure_repo(workdir: Path, *, skip_download: bool) -> Path:
    """
    Ensure the upstream repo exists under `workdir/in-context-learning`.
    Returns repo_dir.
    """
    repo_dir = workdir / "in-context-learning"
    if repo_dir.exists():
        return repo_dir

    if skip_download:
        raise FileNotFoundError(
            f"Repo not found at {repo_dir}. Re-run without --skip-download, or place the repo there."
        )

    print("[1/3] Fetching upstream source repo...")
    zip_path = workdir / "upstream_repo.zip"
    _download(UPSTREAM_REPO_ZIP, zip_path)
    _extract_zip(zip_path, workdir)

    # GitHub zip extracts to a single top-level folder, usually `in-context-learning-main/`
    extracted = None
    for p in workdir.iterdir():
        if p.is_dir() and p.name.startswith("in-context-learning-"):
            extracted = p
            break
    if extracted is None:
        raise RuntimeError("Could not locate extracted repo directory after unzip.")

    # Move into a stable location
    if repo_dir.exists():
        shutil.rmtree(repo_dir)
    extracted.rename(repo_dir)
    return repo_dir


def _ensure_models(repo_dir: Path, workdir: Path, *, skip_models: bool) -> None:
    models_dir = repo_dir / "models"
    if models_dir.exists() and any(models_dir.iterdir()):
        return
    if skip_models:
        print("[2/3] Skipping model checkpoint download (--skip-models).")
        return

    print("[2/3] Fetching pretrained model checkpoints...")
    zip_path = workdir / "models.zip"
    _download(UPSTREAM_MODELS_ZIP, zip_path)
    _extract_zip(zip_path, repo_dir)


def _install_tqdm_notebook_shim() -> None:
    """
    If `tqdm.notebook` cannot be imported (common outside notebooks),
    inject a shim so notebook code that does `from tqdm.notebook import tqdm`
    can still run.
    """
    try:
        import tqdm.notebook  # noqa: F401
        return
    except Exception:
        pass

    try:
        from tqdm.auto import tqdm as _tqdm_auto
    except Exception:
        return

    mod = types.ModuleType("tqdm.notebook")
    mod.tqdm = _tqdm_auto
    sys.modules["tqdm.notebook"] = mod


def main() -> int:
    ap = argparse.ArgumentParser()
    ap.add_argument("--workdir", type=str, default="workspace", help="Working directory (default: ./workspace)")
    ap.add_argument("--skip-download", action="store_true", help="Do not download the upstream repo automatically.")
    ap.add_argument("--skip-models", action="store_true", help="Do not download pretrained model checkpoints.")
    ap.add_argument("--export-only", action="store_true", help="Export Python files but do not execute.")
    args = ap.parse_args()

    project_root = Path(__file__).resolve().parent
    workdir = (project_root / args.workdir).resolve()
    workdir.mkdir(parents=True, exist_ok=True)

    repo_dir = _ensure_repo(workdir, skip_download=args.skip_download)
    _ensure_models(repo_dir, workdir, skip_models=args.skip_models)

    nb_path = repo_dir / "src" / "eval.ipynb"
    if not nb_path.exists():
        raise FileNotFoundError(f"Notebook not found: {nb_path}")

    out_root = workdir / "generated"
    merged_path, parts = export_notebook(nb_path, out_root)

    print(f"[3/3] Exported merged script: {merged_path}")
    print(f"      Exported {len(parts)} cell files under: {out_root / 'eval_ipynb_parts'}")

    if args.export_only:
        return 0

    # Make execution behave like the original notebook environment:
    #  - cwd := repo/src so relative paths like ../models work
    #  - sys.path includes repo/src so `from eval import ...` works
    src_dir = repo_dir / "src"
    os.chdir(src_dir)
    sys.path.insert(0, str(src_dir))

    # Headless-safe matplotlib (if no display)
    if os.environ.get("DISPLAY", "") == "" and os.environ.get("MPLBACKEND", "") == "":
        os.environ["MPLBACKEND"] = "Agg"

    _install_tqdm_notebook_shim()

    # Execute notebook cells in order
    runpy.run_path(str(merged_path), run_name="__main__")
    return 0


if __name__ == "__main__":
    raise SystemExit(main())
